

```mermaid
graph TD
    User --> PlatformUI[Codex-like Platform UI]

    subgraph "Backend Services (Go/Python)"
        APIService[API Service] --> TaskQueue[Task Queue (Kafka)]
        TaskQueue --> WorkerService[Worker Service (Manages K8s Jobs)]
        APIService --> MetadataDB[(MySQL/Postgres - Tasks, Feedback)]
        APIService --> VectorDBAdmin[VectorDB Admin Interface (Optional)]
        FineTuneDataService[Fine-tuning Data Service] --> FineTuneDataStore[(S3/COS - Raw Logs, Processed FT Data)]
    end

    subgraph "Knowledge & Code Indexing Pipeline (Offline/Periodic)"
        CodeSource[User Repos / Public Code] --> CodeParser[Code Parser (Tree-sitter)]
        CodeParser --> ChunkingLogic[Chunking & Structuring]
        ChunkingLogic --> EmbeddingModel[Code Embedding Model (e.g., CodeBERT, UniXcoder, OpenAI Ada)]
        EmbeddingModel --> VectorDB[(Vector DB - FAISS, Pinecone, Weaviate, Milvus)]
        CodeParser --> GraphDB[(Graph DB - Neo4j, for Code Structure/AST - Optional for GraphRAG)]
    end

    subgraph "Agent Core Logic (Python - Runs in K8s Pod)"
        AgentOrchestrator[Agent Orchestrator (MCP & CoT Logic)]

        subgraph "RAG Module"
            QueryPlannerRAG[RAG Query Planner]
            Retriever[Retriever (VectorDB Query, Keyword Search, Graph Traversal)]
            ReRanker[Re-Ranker (Cross-encoder / LLM)]
            ContextBuilderRAG[RAG Context Builder]
        end

        subgraph "LLM Interaction Module"
            LLMClient[LLM Client (Fine-tuned SWE-like model, Base model for CoT/Critique)]
            PromptEngine[Prompt Engineering Logic]
        end

        subgraph "Tool Execution Module (Function Calling)"
            ToolRegistry[Tool Registry & Dispatcher]
            LinterTool[Linter Tool Adapter]
            TestRunnerTool[Test Runner Adapter]
            FileEditorAST[File Editor (AST-based)]
            FileEditorText[File Editor (Text/Diff-based)]
            SearchTool[Web Search Tool Adapter]
            CommandRunner[General Command Runner]
        end

        UserCodeVolume[/app/code - User's Code]
        ASTCache[AST Cache (for frequently accessed files)]
    end

    subgraph "External Services & Models"
        OpenAI_API[OpenAI API (Embeddings, Base LLMs)]
        OSS_LLM_FineTuned[Self-hosted Fine-tuned OSS LLM (e.g., SWE-bench fine-tuned CodeLlama)]
        OSS_Embedding_Model[Self-hosted Embedding Model (Optional)]
    end


    %% Data Flow for a Task
    PlatformUI -- Task Request --> APIService
    WorkerService -- Spawns --> AgentOrchestrator

    AgentOrchestrator -- Initial Analysis / Query Planning --> QueryPlannerRAG
    QueryPlannerRAG -- Query --> Retriever
    Retriever -- Semantic Search --> VectorDB
    Retriever -- Keyword Search (Optional) --> CodeSource % Or an inverted index
    Retriever -- Graph Traversal (Optional) --> GraphDB
    Retriever -- Retrieved Chunks --> ReRanker
    ReRanker -- Ranked Chunks --> ContextBuilderRAG
    ContextBuilderRAG -- Augmented Context --> AgentOrchestrator

    AgentOrchestrator -- CoT Prompt / Task --> LLMClient
    LLMClient -- API Call --> OSS_LLM_FineTuned
    LLMClient -- API Call for Critique/Reasoning --> OpenAI_API % Or another instance of OSS_LLM
    OSS_LLM_FineTuned -- Response (Thought, Tool Call, Code) --> LLMClient
    LLMClient -- Parsed Response --> AgentOrchestrator

    AgentOrchestrator -- Tool Instruction --> ToolRegistry
    ToolRegistry -- Dispatch --> LinterTool
    ToolRegistry -- Dispatch --> TestRunnerTool
    ToolRegistry -- Dispatch --> FileEditorAST
    ToolRegistry -- Dispatch --> SearchTool
    LinterTool -- Executes Linter --> UserCodeVolume
    FileEditorAST -- Modifies AST --> UserCodeVolume
    FileEditorAST -- Updates --> ASTCache
    SearchTool -- External API Call --> Internet[Internet Search Engine]

    ToolRegistry -- Tool Result --> AgentOrchestrator

    AgentOrchestrator -- Final Code/Diff/PR --> PlatformUI % Via Backend
    AgentOrchestrator -- Logs for Fine-tuning --> FineTuneDataService

    %% Fine-tuning Loop
    FineTuneDataStore <-- User Feedback & Agent Logs -- PlatformUI
    FineTuneDataStore --> TrainingPipeline[OSS LLM Fine-tuning Pipeline (e.g., Axolotl, Llama-Factory)]
    TrainingPipeline -- Updates Model Weights --> OSS_LLM_FineTuned
```

---

## I. é«˜çº§ RAG (Retrieval Augmented Generation)

**ç›®æ ‡ï¼š** ä¸º LLM æä¾›æœ€ç›¸å…³ã€æœ€ç²¾ç¡®çš„ä»£ç ä¸Šä¸‹æ–‡ï¼Œå³ä½¿ä»£ç åº“éå¸¸åºå¤§ã€‚

1.  **ä»£ç åµŒå…¥æ¨¡å‹ (Code Embedding Models):**
    *   **ä¸“ç”¨æ¨¡å‹ï¼š** `CodeBERT`, `GraphCodeBERT`, `UniXcoder`, `StarCoder Embeddings`ã€‚è¿™äº›æ¨¡å‹åœ¨ä»£ç æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œèƒ½æ›´å¥½åœ°ç†è§£ä»£ç çš„è¯­ä¹‰å’Œç»“æ„ã€‚
    *   **é€šç”¨å¼ºåŠ›æ¨¡å‹ï¼š** OpenAI `text-embedding-3-small/large`, `text-embedding-ada-002` æˆ–å…¶ä»– SOTA æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚è™½ç„¶é€šç”¨ï¼Œä½†å¯¹äºä»£ç çš„è‡ªç„¶è¯­è¨€æè¿°å’Œéƒ¨åˆ†ä»£ç ç‰‡æ®µæ•ˆæœä¾ç„¶ä¸é”™ã€‚
    *   **é€‰æ‹©ï¼š** å–å†³äºæ‚¨çš„å…·ä½“éœ€æ±‚ã€æ€§èƒ½å’Œæˆæœ¬ã€‚ä¸“ç”¨ä»£ç æ¨¡å‹é€šå¸¸åœ¨çº¯ä»£ç æ£€ç´¢ä¸Šè¡¨ç°æ›´å¥½ã€‚

2.  **å‘é‡æ•°æ®åº“ (Vector Database):**
    *   **é€‰é¡¹ï¼š** FAISS (åº“), Pinecone (æ‰˜ç®¡), Weaviate (å¼€æº), Milvus (å¼€æº), Qdrant (å¼€æº), ChromaDB (å¼€æº)ã€‚
    *   **ä½œç”¨ï¼š** å­˜å‚¨ä»£ç å—çš„åµŒå…¥å‘é‡ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„è¿‘ä¼¼æœ€è¿‘é‚» (ANN) æœç´¢ã€‚
    *   **å®ç°æ€è·¯ (Indexing):**
        ```python
        # Pseudocode for indexing
        from some_embedding_model import CodeEmbedder
        from some_vector_db_client import VectorDBClient
        from code_parser import parse_and_chunk_code # Uses tree-sitter

        embedder = CodeEmbedder(model_name="unixcoder-base") # Or your chosen model
        vector_db = VectorDBClient(config="...")

        def index_repository(repo_path):
            for file_path, code_content in iter_code_files(repo_path):
                chunks = parse_and_chunk_code(file_path, code_content) # Smart chunking
                for chunk in chunks:
                    # chunk = {"id": "repo/file.py#func_name", "code": "...", "metadata": {...}}
                    embedding = embedder.embed(chunk["code"])
                    vector_db.add(chunk["id"], embedding, chunk["metadata"])
        ```

3.  **å¤æ‚ RAG ç­–ç•¥ï¼š**
    *   **A. æ··åˆæœç´¢ (Hybrid Search):**
        *   **æ¦‚å¿µï¼š** ç»“åˆå‘é‡è¯­ä¹‰æœç´¢å’Œä¼ ç»Ÿçš„ç¨€ç–å‘é‡æœç´¢ï¼ˆå¦‚ BM25/TF-IDF å…³é”®è¯æœç´¢ï¼‰ã€‚
        *   **åŸå› ï¼š** è¯­ä¹‰æœç´¢æ“…é•¿ç†è§£æ„å›¾ï¼Œä½†å¯èƒ½é”™è¿‡ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…ï¼ˆå¦‚ç‰¹å®šå˜é‡åã€API è°ƒç”¨ï¼‰ã€‚
        *   **å®ç°æ€è·¯:**
            ```python
            # Pseudocode for hybrid retrieval
            def hybrid_retrieve(query_text, k_semantic=5, k_keyword=5):
                semantic_results = vector_db.search(embedder.embed(query_text), top_k=k_semantic)
                keyword_results = keyword_index.search(query_text, top_k=k_keyword) # e.g., Elasticsearch

                # Combine and de-duplicate results
                combined_results = combine_and_deduplicate(semantic_results, keyword_results)
                return combined_results
            ```
    *   **B. é‡æ’åº (Re-ranking):**
        *   **æ¦‚å¿µï¼š** åœ¨åˆæ­¥æ£€ç´¢ï¼ˆå¬å›ï¼‰åï¼Œä½¿ç”¨æ›´å¼ºå¤§ï¼ˆé€šå¸¸ä¹Ÿæ›´æ…¢ï¼‰çš„æ¨¡å‹å¯¹å¬å›çš„ Top-N ä¸ªç»“æœè¿›è¡Œé‡æ–°æ’åºï¼Œä»¥æé«˜æœ€ç»ˆä¸Šä¸‹æ–‡çš„ç²¾åº¦ã€‚
        *   **æ¨¡å‹ï¼š**
            *   **Cross-Encoders:** å¦‚ `ms-marco-MiniLM-L-12-v2` æˆ–é’ˆå¯¹ä»£ç è®­ç»ƒçš„ cross-encoderã€‚å®ƒä»¬åŒæ—¶å¤„ç† (query, document) å¯¹ï¼Œæ¯”åŒç¼–ç å™¨ï¼ˆç”¨äºåµŒå…¥ï¼‰æ›´ç²¾ç¡®ã€‚
            *   **LLM Re-ranking:** ä½¿ç”¨ä¸€ä¸ªå°å‹ LLMï¼Œç»™å®ƒ query å’Œæ¯ä¸ªå¬å›çš„æ–‡æ¡£ï¼Œè®©å®ƒåˆ¤æ–­ç›¸å…³æ€§æˆ–æ‰“åˆ†ã€‚
        *   **å®ç°æ€è·¯:**
            ```python
            # Pseudocode for re-ranking
            from some_cross_encoder import CrossEncoder

            reranker = CrossEncoder(model_name="...") # Or an LLM client

            def rerank_results(query_text, initial_results):
                query_doc_pairs = [(query_text, doc["code"]) for doc in initial_results]
                scores = reranker.predict(query_doc_pairs) # Or LLM judges relevance

                for i, doc in enumerate(initial_results):
                    doc["rerank_score"] = scores[i]

                sorted_results = sorted(initial_results, key=lambda x: x["rerank_score"], reverse=True)
                return sorted_results[:TOP_K_FINAL] # Select final top K
            ```
    *   **C. å›¾ RAG (Graph RAG):**
        *   **æ¦‚å¿µï¼š** åˆ©ç”¨ä»£ç çš„å›¾ç»“æ„ï¼ˆè°ƒç”¨å›¾ã€ç»§æ‰¿å›¾ã€æ–‡ä»¶ä¾èµ–å›¾ï¼‰æ¥æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚å¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠåˆ°å‡½æ•° Aï¼Œé‚£ä¹ˆè°ƒç”¨å‡½æ•° A çš„å‡½æ•°ã€å‡½æ•° A è°ƒç”¨çš„å‡½æ•°ã€ä»¥åŠä¸å‡½æ•° A åœ¨åŒä¸€æ¨¡å—ä¸­å®šä¹‰çš„ç›¸å…³ç±»/å‡½æ•°éƒ½å¯èƒ½æ˜¯ç›¸å…³çš„ã€‚
        *   **å®ç°æ€è·¯:**
            1.  **æ„å»ºä»£ç å›¾ï¼š** ä½¿ç”¨ `tree-sitter` ç­‰å·¥å…·è§£æä»£ç ï¼Œæå–å‡½æ•°å®šä¹‰ã€è°ƒç”¨ã€ç±»ç»§æ‰¿ç­‰å…³ç³»ï¼Œå­˜å…¥å›¾æ•°æ®åº“ (å¦‚ Neo4j) æˆ–å†…å­˜ä¸­çš„å›¾ç»“æ„ã€‚
            2.  **æ£€ç´¢ï¼š**
                *   **ç§å­èŠ‚ç‚¹ï¼š** è¯†åˆ«ç”¨æˆ·é—®é¢˜æˆ–ç›®æ ‡ä»£ç ä¸­çš„å…³é”®å®ä½“ï¼ˆå‡½æ•°åã€ç±»åï¼‰ã€‚
                *   **å›¾éå†ï¼š** ä»ç§å­èŠ‚ç‚¹å¼€å§‹ï¼Œåœ¨ä»£ç å›¾ä¸Šè¿›è¡Œéå†ï¼ˆå¦‚ BFS, DFS, PageRank-likeï¼‰ï¼Œæ”¶é›†é‚»è¿‘çš„ã€æœ‰å¼ºä¾èµ–å…³ç³»çš„èŠ‚ç‚¹ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
            ```python
            # Pseudocode for GraphRAG
            from some_graph_db_client import GraphDBClient

            graph_db = GraphDBClient(config="...")

            def graph_retrieve(entity_name, graph_query_depth=2):
                # Query graph_db for nodes connected to entity_name
                # e.g., "MATCH (n)-[*1..{graph_query_depth}]-(m) WHERE n.name = '{entity_name}' RETURN m.code"
                connected_code_snippets = graph_db.query_connected_code(entity_name, depth=graph_query_depth)
                return connected_code_snippets
            ```
        *   **ä¸å‘é‡ RAG ç»“åˆï¼š** å¯ä»¥å…ˆç”¨å‘é‡ RAG æ‰¾åˆ°ä¸€äº›åˆå§‹ç›¸å…³çš„ä»£ç å—ï¼Œç„¶åä»è¿™äº›å—ä¸­æå–å®ä½“ï¼Œå†ç”¨å›¾ RAG æ‰©å±•ä¸Šä¸‹æ–‡ã€‚

## II. CoT (Chain-of-Thought) ä¸è¿­ä»£ä¼˜åŒ–

**ç›®æ ‡ï¼š** è®© LLM è¿›è¡Œæ›´å¤æ‚çš„æ¨ç†ã€è§„åˆ’ã€è‡ªæˆ‘æ‰¹åˆ¤å’Œè¿­ä»£æ”¹è¿›ï¼Œè€Œä¸ä»…ä»…æ˜¯å•è½®é—®ç­”ã€‚

**æ¦‚å¿µï¼š** CoT æç¤ºå¼•å¯¼ LLM "ä¸€æ­¥ä¸€æ­¥åœ°æ€è€ƒ" æˆ– "å¤§å£°æ€è€ƒ"ã€‚åœ¨ MCP (Model-Critique-Prompt) å¾ªç¯ä¸­ï¼ŒCoT æ˜¯æ ¸å¿ƒã€‚

**å®ç°æ€è·¯ (èå…¥ AgentOrchestrator):**

```python
# Pseudocode within AgentOrchestrator or MCP logic
class AgentOrchestrator:
    def __init__(self, llm_client, tool_registry, task_id, initial_prompt_template):
        self.llm = llm_client
        self.tools = tool_registry
        self.task_id = task_id
        self.prompt_template = initial_prompt_template # Template for CoT
        self.history = [] # Stores (thought, action, observation) tuples

    def run_mcp_loop(self, user_query, code_context):
        current_goal = user_query
        for i in range(MAX_MCP_ITERATIONS):
            # 1. Thought/Planning (CoT)
            prompt = self.prompt_template.format(
                goal=current_goal,
                code_context=code_context,
                history=self.history_to_string(), # Convert history to string for prompt
                available_tools=self.tools.get_schema_string()
            )
            # The prompt should ask the LLM to:
            # - Analyze the goal and current state.
            # - Formulate a plan (chain of thought).
            # - Decide on the next immediate action (call a tool or provide an answer).
            # - If calling a tool, specify tool name and arguments.
            # - If providing an answer/code, ensure it's the final step.

            llm_response_text = self.llm.generate(prompt, model="OSS_LLM_FineTuned_or_Base_for_CoT")
            self.log_interaction("thought_prompt", prompt)
            self.log_interaction("llm_thought_response", llm_response_text)

            # 2. Parse Action from LLM's thought process
            action_name, action_args = self.parse_action_from_llm_response(llm_response_text)

            if action_name == "FINAL_ANSWER" or action_name == "FINAL_CODE":
                final_output = action_args.get("content", llm_response_text)
                self.log_interaction("final_output", final_output)
                return final_output # Task complete

            # 3. Execute Action (Tool Call)
            if action_name and self.tools.has_tool(action_name):
                observation = self.tools.execute(action_name, action_args)
                self.log_interaction("action_executed", {"name": action_name, "args": action_args})
                self.log_interaction("observation", observation)
                self.history.append({"thought": llm_response_text, "action": (action_name, action_args), "observation": observation})
            else:
                # LLM didn't call a valid tool, or called a non-existent one.
                # Could be an error, or the LLM is just reasoning.
                observation = "No valid tool was called. LLM might be reasoning or an error occurred."
                self.log_interaction("no_tool_called", observation)
                self.history.append({"thought": llm_response_text, "action": None, "observation": observation})


            # 4. Critique (Implicit or Explicit) & Refine Goal for next iteration
            # The next prompt iteration will include the history, allowing the LLM to critique its own previous steps.
            # Or, you can have an explicit critique step:
            # critique_prompt = f"Previous plan: {llm_response_text}\nAction taken: {action_name}\nObservation: {observation}\nIs this progress good? What should be adjusted for the next step to achieve: {user_query}?"
            # critique = self.llm.generate(critique_prompt, model="Critique_LLM_or_Base")
            # current_goal = f"Original goal: {user_query}. Critique of last step: {critique}. Refined goal for next step: ..."
            # For simplicity, the history itself serves as implicit critique for the next CoT iteration.

            # Update code_context if a file editing tool was called and successful
            if action_name == "edit_file" and observation.get("status") == "success":
                # Re-fetch or update the relevant part of code_context
                code_context = self.update_code_context_after_edit(action_args.get("file_path"))


        return "Max iterations reached. Unable to complete task."

    def history_to_string(self):
        # Convert self.history to a string format suitable for the LLM prompt
        # ... implementation ...
        pass

    def parse_action_from_llm_response(self, llm_text):
        # Use regex, JSON parsing, or specific markers in LLM output
        # to extract tool_name and tool_args.
        # Example: LLM might output:
        # "Thought: I need to check the syntax of main.py.
        #  Action: { \"tool_name\": \"run_linter\", \"arguments\": {\"file_path\": \"main.py\"} }"
        # ... implementation ...
        pass
```
**CoT Prompting Example Snippet for `self.prompt_template`:**
```text
You are an expert AI programmer. Your current goal is: {goal}

Available tools:
{available_tools}

Code Context:
{code_context}

Previous Steps (Thought, Action, Observation):
{history}

Based on the goal, context, and history:
1.  **Think step-by-step (Chain of Thought):** Analyze the current situation. What needs to be done next? What information is missing?
2.  **Decision:**
    *   If you have enough information and can complete the goal OR generate the required code, provide the final answer directly. If providing code, clearly delimit it. Prefix your final answer with "FINAL_ANSWER:" or "FINAL_CODE:".
    *   If you need to use a tool, specify the tool call as a JSON object: { "tool_name": "tool_name_here", "arguments": {"arg1": "value1", ...} }. Only output this JSON if you are calling a tool.
    *   If you are just thinking or planning further, explain your thoughts.

Your response:
```

## III. å·¥å…·ä½¿ç”¨/å‡½æ•°è°ƒç”¨ (MCP - Model-Context-Protocol)

è¿™éƒ¨åˆ†ä¸ CoT ç´§å¯†ç›¸å…³ã€‚AgentOrchestrator ä¼šè§£æ LLM çš„ CoT è¾“å‡ºï¼Œå¦‚æœåŒ…å«å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œåˆ™æ‰§è¡Œã€‚

**`ToolRegistry` å’Œå·¥å…·é€‚é…å™¨ (Adapters):**
*   `ToolRegistry` ç»´æŠ¤ä¸€ä¸ªå¯ç”¨å·¥å…·çš„åˆ—è¡¨åŠå…¶ schema (ç”¨äºå‘ LLM å£°æ˜)ã€‚
*   æ¯ä¸ªå·¥å…· (Linter, Test Runner, File Editor, Search) éƒ½æœ‰ä¸€ä¸ªé€‚é…å™¨å±‚ï¼Œè´Ÿè´£ï¼š
    *   æ¥æ”¶æ¥è‡ª Orchestrator çš„å‚æ•°ã€‚
    *   æ‰§è¡Œå®é™…çš„å·¥å…·é€»è¾‘ (è°ƒç”¨ `subprocess`, `requests` API, ASTåº“)ã€‚
    *   å°†å·¥å…·çš„åŸå§‹è¾“å‡ºæ ¼å¼åŒ–ä¸ºæ ‡å‡†åŒ–çš„ `observation` ç»“æœ (é€šå¸¸æ˜¯ JSON æˆ–æ–‡æœ¬æ‘˜è¦) è¿”å›ç»™ Orchestratorã€‚

```python
# Pseudocode for ToolRegistry
class ToolRegistry:
    def __init__(self):
        self.tools = {} # tool_name -> tool_execution_function
        self.tool_schemas = [] # List of OpenAI-compatible function schemas

    def register_tool(self, name, function, schema):
        self.tools[name] = function
        self.tool_schemas.append({"type": "function", "function": schema})

    def has_tool(self, name):
        return name in self.tools

    def execute(self, tool_name, args_dict):
        if tool_name in self.tools:
            try:
                # Potentially sanitize/validate args_dict here
                return self.tools[tool_name](**args_dict)
            except Exception as e:
                return {"error": f"Error executing tool {tool_name}: {str(e)}"}
        return {"error": f"Tool {tool_name} not found."}

    def get_schema_string(self): # For LLM prompt
        return json.dumps(self.tool_schemas, indent=2)

# Example tool registration in Agent initialization:
# tool_registry = ToolRegistry()
# tool_registry.register_tool("run_linter", run_linter_adapter_func, linter_schema_json)
# tool_registry.register_tool("edit_file_ast", edit_file_ast_adapter_func, edit_file_ast_schema_json)
```

## IV. æ¨¡å‹å¾®è°ƒ (Fine-tuning OSS Models like SWE-bench models)

**ç›®æ ‡ï¼š** ä½¿å¼€æº LLM (å¦‚ CodeLlama, StarCoder, DeepSeek Coderï¼Œç‰¹åˆ«æ˜¯é‚£äº›åœ¨ SWE-bench ç­‰åŸºå‡†ä¸Šè¡¨ç°å¥½çš„æ¨¡å‹) æ›´æ“…é•¿æ‚¨çš„ç‰¹å®šä»»åŠ¡ç±»å‹ã€éµå¾ªæ‚¨çš„ä»£ç é£æ ¼ã€æˆ–æ›´å¥½åœ°è¿›è¡Œ CoT æ¨ç†å’Œå·¥å…·è°ƒç”¨ã€‚

1.  **æ•°æ®æ”¶é›† (å…³é”®ï¼):**
    *   **Agent äº¤äº’æ—¥å¿—ï¼š** `(prompt_to_llm, ideal_llm_response_with_cot_and_tool_call)` 
     `ideal_llm_response` å¯èƒ½éœ€è¦äººå·¥ä¿®æ­£æˆ–ç”±éå¸¸å¼ºå¤§çš„æ¨¡å‹ (å¦‚ GPT-o3) ç”Ÿæˆä½œä¸ºæ•™å¸ˆã€‚
    *   **ä»£ç ä¿®æ”¹æ•°æ®ï¼š** `(code_before, user_instruction, code_after_with_agent_help)`ã€‚
    *   **ç”¨æˆ·åé¦ˆï¼š** å°†ç”¨æˆ·å¯¹ Agent ç”Ÿæˆç»“æœçš„è¯„åˆ†ã€ä¿®æ­£ã€è¯„è®ºæ•´åˆè¿›æ¥ã€‚
    *   **æ ¼å¼ï¼š** é€šå¸¸æ˜¯ JSONLï¼Œæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œæ ¼å¼éµå¾ªæ‰€é€‰å¾®è°ƒæ¡†æ¶çš„è¦æ±‚ (å¦‚ Alpaca æ ¼å¼ï¼ŒShareGPT æ ¼å¼)ã€‚
        ```json
        // Example for instruction fine-tuning (Alpaca-like)
        {
            "instruction": "Refactor the given Python function to use a list comprehension and improve readability.",
            "input": "def get_squares(n):\n  sq = []\n  for i in range(n):\n    sq.append(i*i)\n  return sq",
            "output": "def get_squares(n):\n  \"\"\"Returns a list of squares up to n using list comprehension.\"\"\"\n  return [i*i for i in range(n)]"
        }
        // Example for CoT/Tool fine-tuning (ChatML-like)
        {
            "messages": [
                {"role": "system", "content": "You are a helpful AI assistant..."},
                {"role": "user", "content": "How do I check for syntax errors in main.py?"},
                {"role": "assistant", "content": "Thought: I should use the linter tool to check main.py.\nAction: {\"tool_name\": \"run_linter\", \"arguments\": {\"file_path\": \"main.py\"}}"}
            ]
        }
        ```

2.  **å¾®è°ƒæ¡†æ¶ï¼š**
    *   **Axolotl:** æµè¡Œä¸”çµæ´»ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œæ•°æ®é›†æ ¼å¼ã€‚
    *   **LLaMA-Factory (PEFT):** æ˜“äºä¸Šæ‰‹ï¼Œæ”¯æŒå¤šç§ PEFT æ–¹æ³• (LoRA, QLoRA)ã€‚
    *   Hugging Face `transformers` Trainer: æ›´åº•å±‚ï¼Œä½†æä¾›å®Œå…¨æ§åˆ¶ã€‚

3.  **å¾®è°ƒæŠ€æœ¯ï¼š**
    *   **Full Fine-tuning:** è®­ç»ƒæ‰€æœ‰æ¨¡å‹å‚æ•°ï¼ˆè®¡ç®—å¯†é›†ï¼‰ã€‚
    *   **PEFT (Parameter-Efficient Fine-Tuning):**
        *   **LoRA/QLoRA:** åªè®­ç»ƒå°‘é‡é¢å¤–æ·»åŠ çš„å‚æ•°ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶èƒ½è¾¾åˆ°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚è¿™æ˜¯ç›®å‰å¾®è°ƒå¼€æº LLM çš„ä¸»æµæ–¹æ³•ã€‚

4.  **è¿­ä»£ï¼š** å¾®è°ƒæ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚æ”¶é›†æ›´å¤šæ•°æ®ï¼Œé‡æ–°å¾®è°ƒï¼Œè¯„ä¼°ï¼Œéƒ¨ç½²ã€‚

## V. AST æ“ä½œ (Abstract Syntax Tree)

**ç›®æ ‡ï¼š** å¯¹äºç»“æ„åŒ–çš„ä»£ç é‡æ„ä»»åŠ¡ï¼ˆå¦‚å˜é‡é‡å‘½åã€å‡½æ•°ç­¾åä¿®æ”¹ã€å®‰å…¨åœ°æ·»åŠ /åˆ é™¤ä»£ç å—ï¼‰ï¼Œç›´æ¥æ“ä½œ AST æ¯”åŸºäºæ–‡æœ¬çš„ diff æ›´å¯é ã€æ›´ç²¾ç¡®ã€‚

1.  **AST è§£æå™¨ï¼š**
    *   **`tree-sitter`:** å¼ºçƒˆæ¨èã€‚å®ƒæ˜¯ä¸€ä¸ªå¢é‡å¼è§£æåº“ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œå¯ä»¥ç”Ÿæˆå…·ä½“çš„è¯­æ³•æ ‘ã€‚æœ‰ Python, Go, Rust ç­‰è¯­è¨€çš„ç»‘å®šã€‚
    *   ç‰¹å®šè¯­è¨€çš„å†…ç½®åº“ï¼šå¦‚ Python çš„ `ast` æ¨¡å—ã€‚

2.  **LLM è¾“å‡ºç»“æ„åŒ–æŒ‡ä»¤ï¼š**
    *   å¾®è°ƒ LLM æˆ–é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ Promptï¼Œè®©å®ƒè¾“å‡ºæè¿° AST å˜æ›´çš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ JSONï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´ä»£ç ã€‚
    *   **ç¤ºä¾‹æŒ‡ä»¤ (JSON):**
        ```json
        {
            "file_path": "src/utils.py",
            "actions": [
                {
                    "type": "rename_variable",
                    "function_scope": "calculate_total",
                    "old_name": "temp_sum",
                    "new_name": "running_total"
                },
                {
                    "type": "change_function_signature",
                    "function_name": "process_data",
                    "new_parameters": [{"name": "data", "type": "List[int]"}, {"name": "config", "type": "Dict", "default_value": null}],
                    "new_return_type": "Optional[str]"
                },
                {
                    "type": "add_decorator",
                    "function_name": "get_user_info",
                    "decorator_name": "@cache_results"
                }
            ]
        }
        ```

3.  **AST ä¿®æ”¹é€»è¾‘ (`FileEditorAST` å·¥å…·):**
    *   è¿™ä¸ªå·¥å…·æ¥æ”¶ä¸Šè¿° JSON æŒ‡ä»¤ã€‚
    *   ä½¿ç”¨ `tree-sitter` (æˆ–ç­‰æ•ˆåº“) è§£æç›®æ ‡æ–‡ä»¶åˆ° ASTã€‚
    *   éå† ASTï¼Œå®šä½åˆ°éœ€è¦ä¿®æ”¹çš„èŠ‚ç‚¹ã€‚
    *   æ‰§è¡Œç›¸åº”çš„ AST å˜æ¢ (é‡å‘½åèŠ‚ç‚¹ã€æ·»åŠ /åˆ é™¤å­èŠ‚ç‚¹ã€ä¿®æ”¹èŠ‚ç‚¹å±æ€§)ã€‚
    *   å°†ä¿®æ”¹åçš„ AST è½¬æ¢å›ä»£ç æ–‡æœ¬ã€‚
    *   **æŒ‘æˆ˜ï¼š** AST æ“ä½œæœ¬èº«å¯èƒ½å¾ˆå¤æ‚ï¼Œéœ€è¦å¯¹ç‰¹å®šè¯­è¨€çš„è¯­æ³•ç»“æ„æœ‰æ·±å…¥ç†è§£ã€‚ä¿æŒä»£ç æ ¼å¼å’Œæ³¨é‡Šå¯èƒ½ä¹Ÿéœ€è¦é¢å¤–å¤„ç†ã€‚

```python
# Pseudocode for FileEditorAST tool using tree-sitter (conceptual)
from tree_sitter import Language, Parser
# Assume Python language grammar is available: PY_LANGUAGE = Language('build/my-languages.so', 'python')

class ASTEditorTool:
    def __init__(self):
        # self.parser = Parser()
        # self.parser.set_language(PY_LANGUAGE) # Example for Python
        pass # Initialize parser for the target language based on file extension

    def _get_parser_for_file(self, file_path):
        # Dynamically load tree-sitter grammar based on file extension
        # ... placeholder ...
        if file_path.endswith(".py"):
            # parser = Parser()
            # parser.set_language(PY_LANGUAGE)
            # return parser
            pass
        raise ValueError(f"Unsupported language for AST operations: {file_path}")


    def apply_ast_changes(self, file_path: str, actions: list):
        full_file_path = CODE_DIR / file_path
        # parser = self._get_parser_for_file(file_path) # Get appropriate parser

        with open(full_file_path, "rb") as f: # tree-sitter often works with bytes
            code_bytes = f.read()

        # tree = parser.parse(code_bytes)
        # root_node = tree.root_node
        # editable_code = list(code_bytes) # Or use a library that supports AST to text with edits

        # For each action in `actions`:
        #   Find the relevant node(s) in `root_node` using tree-sitter queries or traversal.
        #   Perform the modification. This is highly language and action-specific.
        #   Example: For rename_variable, find all identifier nodes with old_name within scope,
        #            then replace their text content.
        #   Tree-sitter's edit mechanism or rebuilding parts of the text from modified nodes.
        #   This part is complex and requires deep tree-sitter API knowledge for each action type.

        # new_code_bytes = self.ast_to_code(modified_root_node_or_edits) # Convert back
        # with open(full_file_path, "wb") as f:
        #    f.write(new_code_bytes)

        logging.warning("AST editing is highly complex. This is a conceptual placeholder.")
        return {"status": "success_placeholder", "message": "AST changes (conceptually) applied."}

    def ast_to_code(self, ast_node_or_edits):
        # Convert modified AST back to source code text, preserving formatting if possible.
        # ... placeholder ...
        pass
```


ä¸‹é¢æŠŠã€Œé«˜çº§ RAGï¼‹CoTï¼‹MCP å¾ªç¯ã€çš„å‰©ä½™éƒ¨åˆ†è¡¥å®Œï¼Œå¹¶ç»™å‡ºä¸€æ¡ä»â€œæ—¥å¿—é‡‡é›†â†’æ•°æ®æ¸…æ´—â†’å¢é‡å¾®è°ƒâ€çš„å®Œæ•´é—­ç¯å‚è€ƒå®ç°ã€‚æ‰€æœ‰å†…å®¹éƒ½ä¿æŒä¸­æ–‡ã€‚ğŸš€

---

## II-B. æ‰¹åˆ¤-æ”¹è¿› (Critique & Refine) â€”â€” å®Œæˆ CoT/MCP é—­ç¯

```python
# 4. Critique & Refine (ç»§ç»­ä¸Šæ®µä¼ªä»£ç )
            # 4-A è°ƒç”¨æ‰¹åˆ¤æ¨¡å‹ï¼Œå¯¹åˆšæ‰ observation ä¸ thought è¿›è¡Œè¯„ä¼°
            critique_prompt = CRITIQUE_TEMPLATE.format(
                goal=current_goal,
                thought=llm_response_text,
                action=action_name,
                observation=observation
            )
            critique_resp = self.llm.generate(
                critique_prompt,
                model="OPENAI_CRITIQUE_MODEL_ID"  # å¯ä¸ä¸»æ¨¡å‹ä¸åŒ
            )
            self.log_interaction("critique", critique_resp)

            # 4-B æ ¹æ®æ‰¹åˆ¤æ„è§æ›´æ–°ä¸‹ä¸€è½®çš„ goal æˆ–åœ¨ prompt ä¸­æ’å…¥ã€Šä¸Šä¸€è½®æ‰¹åˆ¤æ‘˜è¦ã€‹
            current_goal = self.update_goal_with_critique(
                current_goal, critique_resp
            )

            # å¦‚æœæ‰¹åˆ¤æ¨¡å‹ç»™å‡ºâ€œå·²è¾¾æˆç›®æ ‡â€æˆ–â€œæ”¾å¼ƒâ€
            if "terminate" in critique_resp.lower():
                self.log_interaction("terminated_by_critic", critique_resp)
                return observation  # ç›´æ¥ç»“æŸä»»åŠ¡
```

### 1. `CRITIQUE_TEMPLATE`ï¼ˆå¯ç®€å†™ï¼‰

```
ä½ æ˜¯ä¸¥è‹›çš„ä»£ç å®¡æŸ¥æœºå™¨äººã€‚ç›®æ ‡: {goal}
ä¸Šä¸€è½®æ€è€ƒå’ŒåŠ¨ä½œå¦‚ä¸‹:
Thought:\n{thought}\n
Action: {action}\n
Observation:\n{observation}\n
è¯·å®šä½é—®é¢˜æˆ–æ½œåœ¨æ”¹è¿›ç‚¹ï¼Œè‹¥å·²æ»¡è¶³ç›®æ ‡è¯·å› `TERMINATE`ã€‚
æ ¼å¼:
[
  {"severity": "HIGH|MEDIUM|LOW", "comment": "..."},
  ...
]
```

### 2. `update_goal_with_critique`

```python
def update_goal_with_critique(self, goal:str, critique:str) -> str:
    if "terminate" in critique.lower():
        return goal  # æ— éœ€å†æ”¹
    return goal + "\n# è¯„å®¡æ„è§:\n" + critique
```

è¿™æ ·å°±å®ç°äº† **Thought â†’ Action â†’ Observation â†’ Critique â†’ Refine** çš„å®Œæ•´äº”æ­¥å¾ªç¯ã€‚  
å®è·µä¸­æ•ˆæœæœ€å¥½çš„ä¸€èˆ¬è¿­ä»£ 3-6 æ¬¡å³å¯æ”¶æ•›ã€‚

---

## III. å¾®è°ƒæ•°æ®æ”¶é›†ä¸è®­ç»ƒé—­ç¯

1. **æ•°æ®è½ç›˜ï¼ˆå·²åœ¨å‰é¢ `record_iteration` å†™å…¥ JSONLï¼‰ï¼š**  
   æ¯æ¡è®°å½•åŒ…å«ï¼š
   ```
   {
     "iteration": 3,
     "agent_input": {...},
     "mcp_reply": {...}          # å…¶ä¸­å« thought / tool_call / critique
   }
   ```

2. **æ•°æ®æ¸…æ´—è„šæœ¬ï¼ˆç¤ºä¾‹ï¼‰ï¼š**

```bash
python scripts/extract_ft_pairs.py \
    --jsonl_dir /app/output/finetuning_data \
    --out_file ft_dataset.jsonl
```

`extract_ft_pairs.py` è¦åšçš„äº‹ï¼š

```
for æ¯ä¸ª task_id:
    å°† history æŒ‰é¡ºåºä¸²æˆç³»ç»Ÿ / ç”¨æˆ· / åŠ©æ‰‹æ¶ˆæ¯
    æœ€åä¸€è½®è‹¥å« final_answerï¼Œåˆ™å†™å…¥ {"messages":[...]}
```

3. **å¢é‡å¾®è°ƒï¼ˆä»¥ `Axolotl` ä¸ºä¾‹ï¼‰ï¼š**

```bash
accelerate launch -m axolotl.cli.train \
    -c configs/finetune_llama_swe.yaml \
    dataset.path=ft_dataset.jsonl \
    model.pretrained=codellama/CodeLlama-13b \
    output_dir=models/ft-202406
```

   - å¯é€‰æ‹© **LoRA** / **QLoRA** èŠ‚çœæ˜¾å­˜  
   - æ¯æ™šå®šæ—¶è·‘ä¸€æ¬¡ï¼Œäº§å‡ºæ–°æƒé‡åæ»šåŠ¨æ›´æ–° K8s Deployment çš„ `ConfigMap` å³å¯ç°åº¦å‘å¸ƒ

---

## IV. å·¥å…·æ³¨å†Œä¸è‡ªåŠ¨ Schema ç”Ÿæˆ

ç”¨ pydantic / dataclasses æŠŠæ¯ä¸ªå·¥å…·çš„å‚æ•°æè¿°æš´éœ²ç»™ LLMï¼Œå‡å°‘â€œå‚æ•°é”™ä½â€ç°è±¡ã€‚

```python
from pydantic import BaseModel, Field

class EditFileArgs(BaseModel):
    file_path: str = Field(..., desc="ç›¸å¯¹è·¯å¾„")
    new_content: str | None = Field(None)
    diff_patch: str | None = None
    insert_after_line: int = -1
    replace_lines: tuple[int,int] | None = None

TOOL_SCHEMAS = {
    "edit_file": EditFileArgs.schema(),   # è‡ªåŠ¨ç”Ÿæˆ JSON schema
    ...
}
```

åœ¨ `prompt_template` ä¸­æ’å…¥ï¼š

```
å¯ç”¨å·¥å…·åŠå‚æ•° (JSON Schema):
{{tool_schemas}}
```

LLMï¼ˆgpt-4/8k/32kï¼‰å·²æ”¯æŒ `tool`/`function_calling`ï¼Œè¿™æ ·è¿”å›ç»“æ„ä½“å°±èƒ½è¢« `json.loads` ç›´æ¥è§£æã€‚

---

## V. ç›‘æ§ & è§‚æµ‹æ€§

1. **Prometheus + Grafana**  
   - æ‹‰å– `agent.log` ä¸­çš„å…³é”®è¡Œï¼Œå¦‚ `tool_success_total{tool="edit_file"}`  
   - ç»Ÿè®¡å¤±è´¥ç‡ã€å¹³å‡è¿è¡Œæ—¶ã€LLM token ä½¿ç”¨é‡

2. **åˆ†å¸ƒå¼ Tracing**  
   - OpenTelemetry SDKï¼š`trace_id` å†™è¿› `task_id`ï¼Œå¹³å°ç«¯å’Œ Agent ç«¯ä¸²èµ·å…¨é“¾è·¯

3. **Red Team / ä»£ç æ³¨å…¥**  
   - ç¦»çº¿è·‘ä¸€å¥—â€œæ¶æ„æç¤ºâ€é›†ï¼ŒæŸ¥çœ‹ Agent æ˜¯å¦ä¼šæ‰§è¡Œå±é™©å‘½ä»¤  
   - è‹¥å¤±è´¥ç‡>é˜ˆå€¼å³è‡ªåŠ¨ rollback åˆ°ä¸Šä¸€ç‰ˆå¾®è°ƒæ¨¡å‹

---

## VI. éƒ¨ç½²å°è´´å£«

| ç»„ä»¶ | æ¨èå®ä¾‹ç±»å‹ | è‡ªåŠ¨ä¼¸ç¼©æŒ‡æ ‡ |
|------|-------------|-------------|
| APIService | t3.mediumï¼ˆæ— çŠ¶æ€ï¼‰ | QPS /95 å»¶è¿Ÿ |
| WorkerService | gpu-a10-x2 | é˜Ÿåˆ—é•¿åº¦ |
| VectorDB | r6i.large + æœ¬åœ° SSD | æŸ¥è¯¢ QPS |
| Fine-tune Pipeline | spot-gpu | ä½œä¸šæ’é˜Ÿé•¿åº¦ |

---

### ä¸€å¼ æ€»è§ˆå›¾

```mermaid
flowchart LR
    subgraph CloseLoop
        logs[ğŸšœ Logs] --> clean[ğŸ§¹ Clean & Format] --> lora[ğŸ“ˆ LoRA Fine-tune] --> model[ğŸ¯ New Weights] --> deploy[ğŸš€ Canary Deploy]
        deploy -->|ğŸ’¬| AgentPods
        AgentPods -->|JSONL| logs
    end
```
 
â€¢ é«˜å¬å›ã€é«˜ç²¾åº¦çš„ **RAG**  
â€¢ è‡ªæˆ‘æ€è€ƒã€è‡ªæˆ‘æ‰¹åˆ¤çš„ **CoT/MCP**  
â€¢ æŒç»­è¿›åŒ–çš„ **å¾®è°ƒæ¨¡å‹**  

